{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils_ner.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPgMlmqeCRZQN72abrvyoPG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NR9Bakeye15d","executionInfo":{"status":"ok","timestamp":1635501465837,"user_tz":-480,"elapsed":8638,"user":{"displayName":"戴勤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03940531057500885788"}},"outputId":"3cba6e89-76ff-471d-be32-8faa9822aef6"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 3.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 42.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 70.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 57.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.0\n"]}]},{"cell_type":"code","metadata":{"id":"_x6e8L13ekWU"},"source":["import logging\n","import os\n","import pdb\n","from dataclasses import dataclass\n","from enum import Enum\n","from typing import List, Optional, Union\n","\n","from filelock import FileLock\n","\n","from transformers import PreTrainedTokenizer, is_tf_available, is_torch_available\n","\n","\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUYt05-_fG5c"},"source":["@dataclass\n","class InputExample:\n","    \"\"\"\n","    A single training/test example for token classification.\n","\n","    Args:\n","        guid: Unique id for the example.\n","        words: list. The words of the sequence.\n","        labels: (Optional) list. The labels for each word of the sequence. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","\n","    guid: str\n","    words: List[str]\n","    labels: Optional[List[str]]\n","\n","\n","@dataclass\n","class InputFeatures:\n","    \"\"\"\n","    A single set of features of data.\n","    Property names are the same names as the corresponding inputs to a model.\n","    \"\"\"\n","\n","    input_ids: List[int]\n","    attention_mask: List[int]\n","    token_type_ids: Optional[List[int]] = None\n","    label_ids: Optional[List[int]] = None\n","\n","\n","class Split(Enum):\n","    train = \"train_dev\"\n","    dev = \"devel\"\n","    test = \"test\"\n","\n","\n","if is_torch_available():\n","    import torch\n","    from torch import nn\n","    from torch.utils.data.dataset import Dataset\n","\n","    class NerDataset(Dataset):\n","        \"\"\"\n","        This will be superseded by a framework-agnostic approach\n","        soon.\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","        pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n","        # Use cross entropy ignore_index as padding label id so that only\n","        # real label ids contribute to the loss later.\n","\n","        def __init__(\n","            self,\n","            data_dir: str,\n","            tokenizer: PreTrainedTokenizer,\n","            labels: List[str],\n","            model_type: str,\n","            max_seq_length: Optional[int] = None,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            # Load data features from cache or dataset file\n","            cached_features_file = os.path.join(\n","                data_dir, \"cached_{}_{}_{}\".format(mode.value, tokenizer.__class__.__name__, str(max_seq_length)),\n","            )\n","\n","            # Make sure only the first process in distributed training processes the dataset,\n","            # and the others will use the cache.\n","            lock_path = cached_features_file + \".lock\"\n","            with FileLock(lock_path):\n","\n","                if os.path.exists(cached_features_file) and not overwrite_cache:\n","                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n","                    self.features = torch.load(cached_features_file)\n","                else:\n","                    logger.info(f\"Creating features from dataset file at {data_dir}\")\n","                    examples = read_examples_from_file(data_dir, mode)\n","                    # TODO clean up all this to leverage built-in features of tokenizers\n","                    self.features = convert_examples_to_features(\n","                        examples,\n","                        labels,\n","                        max_seq_length,\n","                        tokenizer,\n","                        cls_token_at_end=bool(model_type in [\"xlnet\"]),\n","                        # xlnet has a cls token at the end\n","                        cls_token=tokenizer.cls_token,\n","                        cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n","                        sep_token=tokenizer.sep_token,\n","                        sep_token_extra=False,\n","                        # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                        pad_on_left=bool(tokenizer.padding_side == \"left\"),\n","                        pad_token=tokenizer.pad_token_id,\n","                        pad_token_segment_id=tokenizer.pad_token_type_id,\n","                        pad_token_label_id=self.pad_token_label_id,\n","                    )\n","                    logger.info(f\"Saving features into cached file {cached_features_file}\")\n","                    torch.save(self.features, cached_features_file)\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","if is_tf_available():\n","    import tensorflow as tf\n","\n","    class TFNerDataset:\n","        \"\"\"\n","        This will be superseded by a framework-agnostic approach\n","        soon.\n","        \"\"\"\n","\n","        features: List[InputFeatures]\n","        pad_token_label_id: int = -1\n","        # Use cross entropy ignore_index as padding label id so that only\n","        # real label ids contribute to the loss later.\n","\n","        def __init__(\n","            self,\n","            data_dir: str,\n","            tokenizer: PreTrainedTokenizer,\n","            labels: List[str],\n","            model_type: str,\n","            max_seq_length: Optional[int] = None,\n","            overwrite_cache=False,\n","            mode: Split = Split.train,\n","        ):\n","            examples = read_examples_from_file(data_dir, mode)\n","            # TODO clean up all this to leverage built-in features of tokenizers\n","            self.features = convert_examples_to_features(\n","                examples,\n","                labels,\n","                max_seq_length,\n","                tokenizer,\n","                cls_token_at_end=bool(model_type in [\"xlnet\"]),\n","                # xlnet has a cls token at the end\n","                cls_token=tokenizer.cls_token,\n","                cls_token_segment_id=2 if model_type in [\"xlnet\"] else 0,\n","                sep_token=tokenizer.sep_token,\n","                sep_token_extra=False,\n","                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                pad_on_left=bool(tokenizer.padding_side == \"left\"),\n","                pad_token=tokenizer.pad_token_id,\n","                pad_token_segment_id=tokenizer.pad_token_type_id,\n","                pad_token_label_id=self.pad_token_label_id,\n","            )\n","\n","            def gen():\n","                for ex in self.features:\n","                    if ex.token_type_ids is None:\n","                        yield (\n","                            {\"input_ids\": ex.input_ids, \"attention_mask\": ex.attention_mask},\n","                            ex.label_ids,\n","                        )\n","                    else:\n","                        yield (\n","                            {\n","                                \"input_ids\": ex.input_ids,\n","                                \"attention_mask\": ex.attention_mask,\n","                                \"token_type_ids\": ex.token_type_ids,\n","                            },\n","                            ex.label_ids,\n","                        )\n","\n","            if \"token_type_ids\" not in tokenizer.model_input_names:\n","                self.dataset = tf.data.Dataset.from_generator(\n","                    gen,\n","                    ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n","                    (\n","                        {\"input_ids\": tf.TensorShape([None]), \"attention_mask\": tf.TensorShape([None])},\n","                        tf.TensorShape([None]),\n","                    ),\n","                )\n","            else:\n","                self.dataset = tf.data.Dataset.from_generator(\n","                    gen,\n","                    ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n","                    (\n","                        {\n","                            \"input_ids\": tf.TensorShape([None]),\n","                            \"attention_mask\": tf.TensorShape([None]),\n","                            \"token_type_ids\": tf.TensorShape([None]),\n","                        },\n","                        tf.TensorShape([None]),\n","                    ),\n","                )\n","\n","        def get_dataset(self):\n","            return self.dataset\n","\n","        def __len__(self):\n","            return len(self.features)\n","\n","        def __getitem__(self, i) -> InputFeatures:\n","            return self.features[i]\n","\n","\n","def read_examples_from_file(data_dir, mode: Union[Split, str]) -> List[InputExample]:\n","    if isinstance(mode, Split):\n","        mode = mode.value\n","    file_path = os.path.join(data_dir, f\"{mode}.txt\")\n","    guid_index = 1\n","    examples = []\n","    with open(file_path, encoding=\"utf-8\") as f:\n","        words = []\n","        labels = []\n","        for line in f:\n","          if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","              if words:\n","                  examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n","                  guid_index += 1\n","                  words = []\n","                  labels = []\n","          else:\n","              splits = line.split(\" \")\n","              words.append(splits[0])\n","              if len(splits) > 1:\n","                  splits_replace = splits[-1].replace(\"\\n\", \"\")\n","                  if splits_replace == 'O':\n","                      labels.append(splits_replace)\n","                  else:\n","                      labels.append(splits_replace + \"-bio\")\n","              else:\n","                  # Examples could have no label for mode = \"test\"\n","                  labels.append(\"O\")\n","        if words:\n","            print(\"wordssssssssssssssssssssssssssss:\", words)\n","            examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n","    return examples\n","\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    label_list: List[str],\n","    max_seq_length: int,\n","    tokenizer: PreTrainedTokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-100,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n",") -> List[InputFeatures]:\n","    \"\"\" Loads a data file into a list of `InputFeatures`\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","    # TODO clean up all this to leverage built-in features of tokenizers\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        if ex_index % 10_000 == 0:\n","            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n","\n","        tokens = []\n","        label_ids = []\n","        for word, label in zip(example.words, example.labels):\n","            word_tokens = tokenizer.tokenize(word)\n","            \n","            # bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.\n","            if len(word_tokens) > 0:\n","                tokens.extend(word_tokens)\n","                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = tokenizer.num_special_tokens_to_add()\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\", example.guid)\n","            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n","            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n","\n","        if \"token_type_ids\" not in tokenizer.model_input_names:\n","            segment_ids = None\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids, label_ids=label_ids\n","            )\n","        )\n","    return features\n","\n","\n","def get_labels(path: str) -> List[str]:\n","    if path:\n","        with open(path, \"r\") as f:\n","            labels = f.read().splitlines()\n","            labels = [i+'-bio' if i != 'O' else 'O' for i in labels]\n","        if \"O\" not in labels:\n","            labels = [\"O\"] + labels\n","        return labels\n","    else:\n","        # return [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n","        return [\"O\", \"B-bio\", \"I-bio\"]\n"],"execution_count":null,"outputs":[]}]}