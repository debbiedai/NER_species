{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"run_ner.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLkcc8W4riJRPsk0Km4Xwb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_869GAmVqAu","executionInfo":{"status":"ok","timestamp":1639129625681,"user_tz":-480,"elapsed":3710,"user":{"displayName":"戴勤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03940531057500885788"}},"outputId":"b83ce7f1-7b2e-4645-e6ff-b9879686eaea"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdtQjmUPzX2I","executionInfo":{"status":"ok","timestamp":1639129625681,"user_tz":-480,"elapsed":4,"user":{"displayName":"戴勤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03940531057500885788"}},"outputId":"a25ec729-9a1d-4eca-95de-99cd8ee4b55c"},"source":["%cd \"/content/drive/My Drive/biobert/NER_species\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/biobert/NER_species\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGJJsj2kyCI4","executionInfo":{"status":"ok","timestamp":1639129646624,"user_tz":-480,"elapsed":20945,"user":{"displayName":"戴勤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03940531057500885788"}},"outputId":"303217d0-37ce-42da-b3a2-472e6493bc99"},"source":["!pip install import-ipynb\n","!pip install seqeval\n","!pip install transformers\n","import import_ipynb\n","\n","from utils_ner import NerDataset, Split, get_labels"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","importing Jupyter notebook from utils_ner.ipynb\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"id":"3N2PF6TadpBY","executionInfo":{"status":"ok","timestamp":1639129647056,"user_tz":-480,"elapsed":441,"user":{"displayName":"戴勤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03940531057500885788"}}},"source":["import logging\n","import os\n","import sys\n","import pdb\n","import subprocess\n","\n","from dataclasses import dataclass, field\n","from typing import Dict, List, Optional, Tuple\n","\n","import numpy as np\n","from seqeval.metrics import f1_score, precision_score, recall_score\n","from torch import nn\n","\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForTokenClassification,\n","    AutoModel,\n","    AutoTokenizer,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","from utils_ner import NerDataset, Split, get_labels\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n","    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n","    # or just modify its tokenizer_config.json.\n","    cache_dir: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n","    )\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    data_dir: str = field(\n","        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n","    )\n","    labels: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n","    )\n","    max_seq_length: int = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DqIQWjapd6CJ","executionInfo":{"status":"ok","timestamp":1639129979101,"user_tz":-480,"elapsed":332049,"user":{"displayName":"戴勤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03940531057500885788"}},"outputId":"29c0b01a-ee38-46fc-f446-343583200961"},"source":["def main():\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n","    model_args, data_args, training_args = parser.parse_json_file(json_file='/content/drive/My Drive/biobert/NER_species/args.json')\n","    # parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n","    # if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n","    #     # If we pass only one argument to the script and it's the path to a json file,\n","    #     # let's parse it to get our arguments.\n","    #     model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n","    # else:\n","    #     model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n","\n","    if (\n","        os.path.exists(training_args.output_dir)\n","        and os.listdir(training_args.output_dir)\n","        and training_args.do_train\n","        and not training_args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","        )\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed\n","    set_seed(training_args.seed)\n","\n","    # Prepare CONLL-2003 task\n","    labels = get_labels(data_args.labels)\n","    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n","    num_labels = len(labels)\n","\n","    # Load pretrained model and tokenizer\n","    #\n","    # Distributed training:\n","    # The .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","\n","    config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        num_labels=num_labels,\n","        id2label=label_map,\n","        label2id={label: i for i, label in enumerate(labels)},\n","        cache_dir=model_args.cache_dir,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","        use_fast=model_args.use_fast,\n","    )\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    model_to_save = AutoModel.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","    )\n","    model_to_save.save_pretrained(training_args.output_dir)\n","    tokenizer.save_pretrained(training_args.output_dir)\n","    # import pdb; pdb.set_trace()\n","\n","\n","    # Get datasets\n","    train_dataset = (\n","        NerDataset(\n","            data_dir=data_args.data_dir,\n","            tokenizer=tokenizer,\n","            labels=labels,\n","            model_type=config.model_type,\n","            max_seq_length=data_args.max_seq_length,\n","            overwrite_cache=data_args.overwrite_cache,\n","            mode=Split.train,\n","        )\n","        if training_args.do_train\n","        else None\n","    )\n","    eval_dataset = (\n","        NerDataset(\n","            data_dir=data_args.data_dir,\n","            tokenizer=tokenizer,\n","            labels=labels,\n","            model_type=config.model_type,\n","            max_seq_length=data_args.max_seq_length,\n","            overwrite_cache=data_args.overwrite_cache,\n","            mode=Split.dev,\n","        )\n","        if training_args.do_eval\n","        else None\n","    )\n","\n","    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n","        preds = np.argmax(predictions, axis=2)\n","\n","        batch_size, seq_len = preds.shape\n","\n","        out_label_list = [[] for _ in range(batch_size)]\n","        preds_list = [[] for _ in range(batch_size)]\n","        \n","        for i in range(batch_size):\n","            for j in range(seq_len):\n","                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n","                    out_label_list[i].append(label_map[label_ids[i][j]])\n","                    preds_list[i].append(label_map[preds[i][j]])\n","\n","        return preds_list, out_label_list\n","\n","    def compute_metrics(p: EvalPrediction) -> Dict:\n","        preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n","        \n","        return {\n","            \"precision\": precision_score(out_label_list, preds_list),\n","            \"recall\": recall_score(out_label_list, preds_list),\n","            \"f1\": f1_score(out_label_list, preds_list),\n","        }\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        trainer.train(\n","            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n","        )\n","        trainer.save_model()\n","        # For convenience, we also re-save the tokenizer to the same directory,\n","        # so that you can share your model easily on huggingface.co/models =)\n","        if trainer.is_world_process_zero():\n","            tokenizer.save_pretrained(training_args.output_dir)\n","\n","    # Evaluation\n","    results = {}\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","\n","        result = trainer.evaluate()\n","        \n","        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n","        if trainer.is_world_process_zero():\n","            with open(output_eval_file, \"w\") as writer:\n","                logger.info(\"***** Eval results *****\")\n","                for key, value in result.items():\n","                    logger.info(\"  %s = %s\", key, value)\n","                    writer.write(\"%s = %s\\n\" % (key, value))\n","\n","            results.update(result)\n","    \n","    \n","    # Predict\n","    if training_args.do_predict:\n","        test_dataset = NerDataset(\n","            data_dir=data_args.data_dir,\n","            tokenizer=tokenizer,\n","            labels=labels,\n","            model_type=config.model_type,\n","            max_seq_length=data_args.max_seq_length,\n","            overwrite_cache=data_args.overwrite_cache,\n","            mode=Split.test,\n","        )\n","        print('test_dataset:', test_dataset[0])\n","        predictions, label_ids, metrics = trainer.predict(test_dataset)\n","        preds_list, _ = align_predictions(predictions, label_ids)\n","        \n","        # Save predictions\n","        output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n","        if trainer.is_world_process_zero():\n","            with open(output_test_results_file, \"w\") as writer:\n","                logger.info(\"***** Test results *****\")\n","                for key, value in metrics.items():\n","                    logger.info(\"  %s = %s\", key, value)\n","                    writer.write(\"%s = %s\\n\" % (key, value))\n","\n","        \n","        output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n","        if trainer.is_world_process_zero():\n","            with open(output_test_predictions_file, \"w\") as writer:\n","                with open(os.path.join(data_args.data_dir, \"test.txt\"), \"r\") as f:\n","                    example_id = 0\n","                    for line in f:\n","                        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                            writer.write(line)\n","                            if not preds_list[example_id]:\n","                                example_id += 1\n","                        elif preds_list[example_id]:\n","                            entity_label = preds_list[example_id].pop(0)\n","                            if entity_label == 'O':\n","                                output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n","                            else:\n","                                output_line = line.split()[0] + \" \" + entity_label[0] + \"\\n\"\n","                            # output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n","                            writer.write(output_line)\n","                        else:\n","                            logger.warning(\n","                                \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n","                            )\n","            \n","\n","    return results\n","\n","\n","def _mp_fn(index):\n","    # For xla_spawn (TPUs)\n","    main()\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["12/10/2021 09:47:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/10/2021 09:47:27 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=output_fold/output_v9_t0/runs/Dec10_09-47-27_9f58a7c9255e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=8,\n","output_dir=output_fold/output_v9_t0,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=32,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=output_fold/output_v9_t0,\n","save_on_each_node=False,\n","save_steps=200,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=1,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","12/10/2021 09:47:36 - INFO - utils_ner -   Creating features from dataset file at /content/drive/My Drive/biobert/NER_species/datasets_fold/v9_t0/\n","12/10/2021 09:47:37 - INFO - utils_ner -   Writing example 0 of 1947\n","12/10/2021 09:47:37 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:37 - INFO - utils_ner -   guid: train_dev-1\n","12/10/2021 09:47:37 - INFO - utils_ner -   tokens: [CLS] large gene family expansion ##s and adaptive evolution for odor ##ant and g ##ust ##atory receptors in the p ##ea a ##phi ##d , a ##cy ##rth ##os ##ip ##hon p ##is ##um . [SEP]\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_ids: 101 1415 5565 1266 4298 1116 1105 25031 7243 1111 21430 2861 1105 176 8954 13424 14392 1107 1103 185 4490 170 27008 1181 117 170 3457 11687 2155 9717 8613 185 1548 1818 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   label_ids: -100 2 2 2 2 -100 2 2 2 2 2 -100 2 2 -100 -100 2 2 2 2 -100 2 -100 -100 2 0 -100 -100 -100 -100 -100 1 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:47:37 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:37 - INFO - utils_ner -   guid: train_dev-2\n","12/10/2021 09:47:37 - INFO - utils_ner -   tokens: [CLS] gaining insight into the mechanisms of ch ##em ##ore ##ception in a ##phi ##ds is of primary importance for both in ##te ##gra ##tive studies on the evolution of host plant special ##ization and applied research in p ##est control management because a ##phi ##ds rely on their [SEP]\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_ids: 101 8289 14222 1154 1103 10748 1104 22572 5521 4474 19792 1107 170 27008 3680 1110 1104 2425 4495 1111 1241 1107 1566 14867 3946 2527 1113 1103 7243 1104 2989 2582 1957 2734 1105 3666 1844 1107 185 2556 1654 2635 1272 170 27008 3680 11235 1113 1147 102\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/10/2021 09:47:37 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 -100 -100 -100 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 -100 2 2 2 2 -100 -100 2 2 2 -100\n","12/10/2021 09:47:37 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:37 - INFO - utils_ner -   guid: train_dev-3\n","12/10/2021 09:47:37 - INFO - utils_ner -   tokens: [CLS] sense of smell and taste to locate and assess their host plants . [SEP]\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_ids: 101 2305 1104 4773 1105 5080 1106 12726 1105 15187 1147 2989 3546 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:47:37 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:37 - INFO - utils_ner -   guid: train_dev-4\n","12/10/2021 09:47:37 - INFO - utils_ner -   tokens: [CLS] we made use of the recent genome sequence of the p ##ea a ##phi ##d , a ##cy ##rth ##os ##ip ##hon p ##is ##um , to address the molecular characterization and evolution of key molecular components of ch ##em ##ore ##ception : the odor ##ant ( or [SEP]\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_ids: 101 1195 1189 1329 1104 1103 2793 15519 4954 1104 1103 185 4490 170 27008 1181 117 170 3457 11687 2155 9717 8613 185 1548 1818 117 1106 4134 1103 9546 27419 1105 7243 1104 2501 9546 5644 1104 22572 5521 4474 19792 131 1103 21430 2861 113 1137 102\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/10/2021 09:47:37 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 2 2 2 2 -100 2 -100 -100 2 0 -100 -100 -100 -100 -100 1 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 -100 2 2 -100\n","12/10/2021 09:47:37 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:37 - INFO - utils_ner -   guid: train_dev-5\n","12/10/2021 09:47:37 - INFO - utils_ner -   tokens: [CLS] ) and g ##ust ##atory ( g ##r ) receptor genes . [SEP]\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_ids: 101 114 1105 176 8954 13424 113 176 1197 114 10814 9077 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:37 - INFO - utils_ner -   label_ids: -100 2 2 2 -100 -100 2 2 -100 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:47:40 - INFO - utils_ner -   Saving features into cached file /content/drive/My Drive/biobert/NER_species/datasets_fold/v9_t0/cached_train_dev_BertTokenizer_50\n","12/10/2021 09:47:40 - INFO - utils_ner -   Creating features from dataset file at /content/drive/My Drive/biobert/NER_species/datasets_fold/v9_t0/\n","12/10/2021 09:47:41 - INFO - utils_ner -   Writing example 0 of 256\n","12/10/2021 09:47:41 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:41 - INFO - utils_ner -   guid: devel-1\n","12/10/2021 09:47:41 - INFO - utils_ner -   tokens: [CLS] copper - induced h ##2 ##o ##2 accumulation con ##fers la ##rva ##l tolerance to x ##ant ##hot ##ox ##in by m ##od ##ulating c ##y ##p ##6 ##b ##50 expression in s ##po ##do ##ptera lit ##ura . [SEP]\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_ids: 101 7335 118 10645 177 1477 1186 1477 23168 14255 23085 2495 13461 1233 15745 1106 193 2861 12217 10649 1394 1118 182 5412 10164 172 1183 1643 1545 1830 11049 2838 1107 188 5674 2572 26523 4941 4084 119 102 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   label_ids: -100 2 -100 -100 2 -100 -100 -100 2 2 -100 2 -100 -100 2 2 2 -100 -100 -100 -100 2 2 -100 -100 2 -100 -100 -100 -100 -100 2 2 0 -100 -100 -100 1 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:47:41 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:41 - INFO - utils_ner -   guid: devel-2\n","12/10/2021 09:47:41 - INFO - utils_ner -   tokens: [CLS] in the plant - insect arms race , plants s ##ynth ##esi ##ze toxic compounds to defend against herb ##ivo ##rous insects , whereas insects employ c ##yt ##och ##rome p ##45 ##0 mon ##oo ##xy ##gen ##ases ( p ##45 ##0s ) to de ##to ##xi ##fy [SEP]\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_ids: 101 1107 1103 2582 118 15754 1739 1886 117 3546 188 26588 18766 3171 12844 10071 1106 6472 1222 22245 15435 13149 9895 117 6142 9895 12912 172 25669 9962 11457 185 21336 1568 19863 5658 16844 4915 23105 113 185 21336 13031 114 1106 1260 2430 8745 13268 102\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/10/2021 09:47:41 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   label_ids: -100 2 2 2 -100 -100 2 2 2 2 2 -100 -100 -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 -100 -100 -100 2 -100 -100 2 -100 -100 -100 -100 2 2 -100 -100 2 2 2 -100 -100 -100 -100\n","12/10/2021 09:47:41 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:41 - INFO - utils_ner -   guid: devel-3\n","12/10/2021 09:47:41 - INFO - utils_ner -   tokens: [CLS] these p ##hy ##to ##to ##xin ##s . [SEP]\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_ids: 101 1292 185 7889 2430 2430 16594 1116 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   label_ids: -100 2 2 -100 -100 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:47:41 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:41 - INFO - utils_ner -   guid: devel-4\n","12/10/2021 09:47:41 - INFO - utils_ner -   tokens: [CLS] as u ##bi ##quito ##us environmental con ##tam ##ina ##nts , heavy metals can be easily absorbed by plants and further accumulated in herb ##ivo ##rous insects through the food chains , resulting in tan ##gible consequences for plant - insect interactions . [SEP]\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_ids: 101 1112 190 5567 21594 1361 4801 14255 20284 2983 5240 117 2302 13237 1169 1129 3253 8761 1118 3546 1105 1748 15664 1107 22245 15435 13149 9895 1194 1103 2094 9236 117 3694 1107 15925 12192 8421 1111 2582 118 15754 10393 119 102 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   label_ids: -100 2 2 -100 -100 -100 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 -100 -100 2 2 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:47:41 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:47:41 - INFO - utils_ner -   guid: devel-5\n","12/10/2021 09:47:41 - INFO - utils_ner -   tokens: [CLS] however , whether heavy metals can influence p ##45 ##0 activities and thereby cause further effects on la ##rva ##l tolerance to p ##hy ##to ##to ##xin ##s remains unknown . [SEP]\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_ids: 101 1649 117 2480 2302 13237 1169 2933 185 21336 1568 2619 1105 8267 2612 1748 3154 1113 2495 13461 1233 15745 1106 185 7889 2430 2430 16594 1116 2606 3655 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:47:41 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 2 2 2 -100 -100 -100 -100 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"]},{"output_type":"stream","name":"stdout","text":["wordssssssssssssssssssssssssssss: ['Evolutionary', 'comparisons', 'between', 'these', 'P450', 'genes', 'are', 'the', 'first', 'available', 'for', 'a', 'group', 'of', 'insect', 'genes', 'transcriptionally', 'regulated', 'by', 'hostplant', 'allelochemicals', 'and', 'provide', 'insights', 'into', 'the', 'process', 'by', 'which', 'insects', 'evolve', 'specialized', 'feeding', 'habits', '.']\n"]},{"output_type":"stream","name":"stderr","text":["12/10/2021 09:47:41 - INFO - utils_ner -   Saving features into cached file /content/drive/My Drive/biobert/NER_species/datasets_fold/v9_t0/cached_devel_BertTokenizer_50\n","/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1062: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1947\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 488\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [488/488 05:04, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to output_fold/output_v9_t0/checkpoint-200\n","Configuration saved in output_fold/output_v9_t0/checkpoint-200/config.json\n","Model weights saved in output_fold/output_v9_t0/checkpoint-200/pytorch_model.bin\n","Saving model checkpoint to output_fold/output_v9_t0/checkpoint-400\n","Configuration saved in output_fold/output_v9_t0/checkpoint-400/config.json\n","Model weights saved in output_fold/output_v9_t0/checkpoint-400/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to output_fold/output_v9_t0\n","Configuration saved in output_fold/output_v9_t0/config.json\n","Model weights saved in output_fold/output_v9_t0/pytorch_model.bin\n","tokenizer config file saved in output_fold/output_v9_t0/tokenizer_config.json\n","Special tokens file saved in output_fold/output_v9_t0/special_tokens_map.json\n","12/10/2021 09:52:51 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 256\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='59' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32/32 00:06]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["12/10/2021 09:52:54 - INFO - __main__ -   ***** Eval results *****\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_loss = 0.06866694986820221\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_precision = 0.89\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_recall = 0.7542372881355932\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_f1 = 0.8165137614678899\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_runtime = 2.9823\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_samples_per_second = 85.84\n","12/10/2021 09:52:54 - INFO - __main__ -     eval_steps_per_second = 10.73\n","12/10/2021 09:52:54 - INFO - __main__ -     epoch = 8.0\n","12/10/2021 09:52:54 - INFO - utils_ner -   Creating features from dataset file at /content/drive/My Drive/biobert/NER_species/datasets_fold/v9_t0/\n","12/10/2021 09:52:55 - INFO - utils_ner -   Writing example 0 of 216\n","12/10/2021 09:52:55 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:52:55 - INFO - utils_ner -   guid: test-1\n","12/10/2021 09:52:55 - INFO - utils_ner -   tokens: [CLS] an ##op ##hel ##es step ##hen ##si do ##x - a ##2 shares common ancestry with genes from distant groups of e ##uka ##ryo ##tes encoding a 26 ##s pro ##te ##as ##ome subunit and is in a conserved gene cluster . [SEP]\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_ids: 101 1126 4184 18809 1279 2585 10436 5053 1202 1775 118 170 1477 6117 1887 11626 1114 9077 1121 6531 2114 1104 174 12658 26503 3052 18922 170 1744 1116 5250 1566 2225 6758 27555 1105 1110 1107 170 21996 5565 10005 119 102 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   label_ids: -100 0 -100 -100 -100 1 -100 -100 2 -100 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 2 2 2 -100 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:52:55 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:52:55 - INFO - utils_ner -   guid: test-2\n","12/10/2021 09:52:55 - INFO - utils_ner -   tokens: [CLS] the sequence of a clone ##d an ##op ##hel ##es step ##hen ##si gene showed 72 % in ##ferred amino acid identity with d ##ros ##op ##hil ##a me ##lan ##oga ##ster do ##x - a ##2 and 93 % with its put ##ative or ##th ##olo ##g [SEP]\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_ids: 101 1103 4954 1104 170 22121 1181 1126 4184 18809 1279 2585 10436 5053 5565 2799 5117 110 1107 26025 13736 5190 4193 1114 173 5864 4184 20473 1161 1143 4371 23282 4648 1202 1775 118 170 1477 1105 5429 110 1114 1157 1508 5838 1137 1582 12805 1403 102\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/10/2021 09:52:55 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 -100 0 -100 -100 -100 1 -100 -100 2 2 2 2 2 -100 2 2 2 2 0 -100 -100 -100 -100 1 -100 -100 -100 2 -100 -100 -100 -100 2 2 2 2 2 2 -100 2 -100 -100 -100 -100\n","12/10/2021 09:52:55 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:52:55 - INFO - utils_ner -   guid: test-3\n","12/10/2021 09:52:55 - INFO - utils_ner -   tokens: [CLS] in an ##op ##hel ##es g ##am ##bia ##e . [SEP]\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_ids: 101 1107 1126 4184 18809 1279 176 2312 10242 1162 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   label_ids: -100 2 0 -100 -100 -100 1 -100 -100 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:52:55 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:52:55 - INFO - utils_ner -   guid: test-4\n","12/10/2021 09:52:55 - INFO - utils_ner -   tokens: [CLS] do ##x - a ##2 is the reported but here ##in disputed structural lo ##cus for dip ##hen ##ol o ##xi ##das ##e a ##2 . [SEP]\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_ids: 101 1202 1775 118 170 1477 1110 1103 2103 1133 1303 1394 11807 8649 25338 6697 1111 20866 10436 4063 184 8745 9028 1162 170 1477 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   label_ids: -100 2 -100 -100 -100 -100 2 2 2 2 2 -100 2 2 2 -100 2 2 -100 -100 2 -100 -100 -100 2 -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","12/10/2021 09:52:55 - INFO - utils_ner -   *** Example ***\n","12/10/2021 09:52:55 - INFO - utils_ner -   guid: test-5\n","12/10/2021 09:52:55 - INFO - utils_ner -   tokens: [CLS] database searches identified do ##x - a ##2 related gene sequences from 15 non - insect species from diverse groups . [SEP]\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_ids: 101 8539 18806 3626 1202 1775 118 170 1477 2272 5565 10028 1121 1405 1664 118 15754 1530 1121 7188 2114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","12/10/2021 09:52:55 - INFO - utils_ner -   label_ids: -100 2 2 2 2 -100 -100 -100 -100 2 2 2 2 2 2 -100 -100 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"]},{"output_type":"stream","name":"stdout","text":["wordssssssssssssssssssssssssssss: ['PCR', 'protocol', 'was', 'developed', 'specific', 'to', 'the', 'transgenic', 'sperm', 'DNA', '.']\n"]},{"output_type":"stream","name":"stderr","text":["12/10/2021 09:52:55 - INFO - utils_ner -   Saving features into cached file /content/drive/My Drive/biobert/NER_species/datasets_fold/v9_t0/cached_test_BertTokenizer_50\n","***** Running Prediction *****\n","  Num examples = 216\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["test_dataset: InputFeatures(input_ids=[101, 1126, 4184, 18809, 1279, 2585, 10436, 5053, 1202, 1775, 118, 170, 1477, 6117, 1887, 11626, 1114, 9077, 1121, 6531, 2114, 1104, 174, 12658, 26503, 3052, 18922, 170, 1744, 1116, 5250, 1566, 2225, 6758, 27555, 1105, 1110, 1107, 170, 21996, 5565, 10005, 119, 102, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[-100, 0, -100, -100, -100, 1, -100, -100, 2, -100, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, 2, 2, 2, -100, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100])\n"]},{"output_type":"stream","name":"stderr","text":["12/10/2021 09:52:59 - INFO - __main__ -   ***** Test results *****\n","12/10/2021 09:52:59 - INFO - __main__ -     test_loss = 0.03681797534227371\n","12/10/2021 09:52:59 - INFO - __main__ -     test_precision = 0.7884615384615384\n","12/10/2021 09:52:59 - INFO - __main__ -     test_recall = 0.9111111111111111\n","12/10/2021 09:52:59 - INFO - __main__ -     test_f1 = 0.845360824742268\n","12/10/2021 09:52:59 - INFO - __main__ -     test_runtime = 2.9894\n","12/10/2021 09:52:59 - INFO - __main__ -     test_samples_per_second = 72.254\n","12/10/2021 09:52:59 - INFO - __main__ -     test_steps_per_second = 9.032\n"]}]}]}